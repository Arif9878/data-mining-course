{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Session 01+02: Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practice, which spans two sessions, we will dig down into the basic data mining techniques to have a clear understanding of a dataset and prepare the data as a first stage of a Data Mining project. This process is known as **Data Preparation** or **Data Wrangling**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contents of this session are:\n",
    "\n",
    "0) Preliminaries: <br>\n",
    "- Dataset explanation\n",
    "- Load dataset<br>\n",
    "- Libraries imports<br>\n",
    "\n",
    "1) Exploratory Data Analysis\n",
    "\n",
    "- Calculate basis statistics as mean, median, variance, maximum and minimum <br>\n",
    "- Draw the box plot and identify outliers<br>\n",
    "- Calculate correlations between variables<br>\n",
    "\n",
    "2) Feature engineering:\n",
    "\n",
    "- Deal with missing values<br>\n",
    "- Standardize all numerical columns<br>\n",
    "- Convert categorical columns to dummy binary variables<br>\n",
    "- Date and period management<br>\n",
    "- Feature generation<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset, contained in `device_db.csv` is a 10000 registers of mobile device purchases in a telco company. **Each record in the dataset describes a customer that buys a new mobile telephone**. The attributes are defined as follows:\n",
    " \n",
    "0. PURCHASED_DEVICE: the mobile phone bought by the customer\n",
    "1. DEVICE_VALUE: the cost of the mobile phone bought by the customer\n",
    "2. LAST_DEVICE_DATE: the date of the previous mobile device purchase\n",
    "3. DATA_TRAFFIC_MONTH_(1..6): The Mbps of data traffic in the month (-1...-6) used by the customer previous to the mobile device purchase\n",
    "4. VOICE_TRAFFIC_MONTH_(1..6): The minutes of voice traffic in the month (-1...-6) used by the customer previous to the mobile device purchase\n",
    "5. BILLING_MONTH_(1..6): Billing (USD) in the month (-1...-6) paid by the customer previous to the mobile device purchase\n",
    "6. DEVICE_COST_MONTH_(1..6): Monthly cost (USD) associated to the mobile device finance in the month (-1...-6) paid by the customer previous to the mobile device purchase: proportion of owner-occupied units built prior to 1940\n",
    "7. LINE_ACTIVATION_DATE: Date of the activation of the mobile line by the customer\n",
    "8. MONTHS_LAST_DEVICE: Number of months of the previous mobile device\n",
    "9. DURATION_LINE: Number of months since the customer contracted the mobile line\n",
    "10. PREVIOUS_DEVICE_MODEL: Model of the previous mobile phone\n",
    "11. PREVIOUS_DEVICE_BRAND: Brand of the previous mobile phone\n",
    "\n",
    "This dataset will be used in next practices as recommendation engines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "from datetime import date\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3. Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the csv with separator \",\" and assign to a dataframe variable (use read_csv from Pandas library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dataset = pd.read_csv(\"device_db.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Exploratory data analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Exploratory Data Analysis (EDA) allows to us to have an understanding of the dataset from a stadistics perspective, i.e. data distribution and correlation between variables. This is crucial to select the most valuable variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**REPORT**] Create a table with the header and first 5 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**REPORT**] Describe the dimension of the dataset (number of rows and columns) and the type of the given variables (float, string, integer, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**REPORT**] List all columns that have at least one *NaN* value.\n",
    "\n",
    "Tip: Use the [isna()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.isna.html) function, as explained, e.g., [here](https://medium.com/dunder-data/finding-the-percentage-of-missing-values-in-a-pandas-dataframe-a04fa00f84ab)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**REPORT**] Create a table with statistics including: [mean](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mean.html), [median](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.median.html), [standard deviation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.std.html), [maximum](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.max.html) and [minimum](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.min.html) of each numeric (e.g., *type=float64*) variable.\n",
    "\n",
    "To iterate through the columns of dataframe `df`, you can use `for column_name in df.columns`. To determine the data type of a column, you can use `df[column_name].dtype`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**REPORT**] Compare the previous results for **DEVICE_VALUE**, **DATA_TRAFFIC_MONTH_1**, **VOICE_TRAFFIC_MONTH_1**, **BILLING_MONTH_1**, **DEVICE_COST_MONTH_1**, **MONTHS_LAST_DEVICE** and **DURATION_LINE**  with the ones from the [scipy.stats.describe](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.describe.html) function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**REPORT**] Generate two tables, one with a census of *PURCHASED_DEVICE* and one with a census of *PREVIOUS_DEVICE_MODEL*. A \"census\" is a table that includes, for each possible value of a variable, how many rows have that value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0. Missing values management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way nulls is managed varies according to the meaning of each variable. In some occasions, records should be removed, completed with values from other columns or somehow calculated.\n",
    "\n",
    "Please note that the process of *NaN* handling is stacked, so the output of step (a) should be used as input for step (b), and the output of step (b) should be used as input for step (c)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**CODE**] (a) Eliminate all rows that have a *NaN* value in the column **PURCHASED_DEVICE**, **DEVICE_VALUE**, **LINE_ACTIVATION_DATE**, **PREVIOUS_DEVICE_MODEL** or **PREVIOUS_DEVICE_BRAND**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**CODE**] (b) Replace *NaN* values by 0 in the columns **DATA\\_TRAFFIC\\_MONTH\\_(1..6)**, **VOICE\\_TRAFFIC\\_MONTH\\_(1..6)**, **BILLING\\_MONTH\\_(1..6)** and **DEVICE\\_COST\\_MONTH\\_(1..6)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**CODE**] (c) Assign **LINE_ACTIVATION_DATE** value to the **LAST_DEVICE_CHANGE** in all cases in which the latter is *NaN*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**REPORT**] Create a table with the header and first 5 rows after the previous transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**REPORT**] Describe the dimension of the new dataset (number of rows and columns). List all columns containing *NaN* values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Distributions, outliers, and correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**REPORT**] Plot a histogram of each of **DEVICE_VALUE**, **DATA_TRAFFIC_MONTH_1-3**, **VOICE_TRAFFIC_MONTH_1-3**, **BILLING_MONTH_1-3**, **DEVICE_COST_MONTH_1-3**, **DURATION_LINE**,   . Can you recognize any specific distribution (exponential, bimodal...)?\n",
    "\n",
    "Tip: use [seaborn.distplot](https://seaborn.pydata.org/generated/seaborn.distplot.html) with `kde=False` to create a histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**CODE**] For exponential distribution variables, a \"normalization\" is usually recommended. Apply **log(x)** to **VOICE_TRAFFIC_MONTH_1** and plot its new distribution.\n",
    "\n",
    "[**REPORT**] Include the plot of the new distribution and compare to the previous one, perhaps placing them side-by-side in your report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**REPORT**] Plot the boxplot for **DATA_TRAFFIC_MONTH_1**, **VOICE_TRAFFIC_MONTH_1** and **BILLING_MONTH_1**.  From which value can we consider OUTLIERS for each variable?\n",
    "\n",
    "Tip: use [pandas.DataFrame.plot](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html) with `kind='box'` to create a boxplot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**REPORT**] Calculate the correlation between all of traffic attributes (i.e. voice and data), duration line, billing, device cost and device value variables and comment the results. Which are the variables with more and less correlation with respect to the DEVICE_VALUE variable? Why?\n",
    "\n",
    "Tip: use [pandas.DataFrame.corr](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html) to compute a correlation matrix, and [matplotlib.pyplot.matshow](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.matshow.html) to show this graphically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Date management and period calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**CODE**] Update the **MONTHS_LAST_DEVICE** value as the difference, in months, between **LAST_DEVICE_CHANGE** and today\n",
    "\n",
    "Tip: use `date.today()` to obtain the current date, and [pandas.to_datetime](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html) to convert dates into a numerical representation, to do arithmetic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**CODE**] Update the **DURATION_LINE** value as the difference, in days, between **LINE_ACTIVATION_DATE** and today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Standarization of numerical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing a dataset involves rescaling the distribution of values so that the mean of observed values is 0 and the standard deviation is 1.\n",
    "\n",
    "[**CODE**] Standardize the **DATA_TRAFFIC_MONTH_1**, **VOICE_TRAFFIC_MONTH_1**, **BILLING_MONTH_1** and **DEVICE_COST_MONTH_1** columns. Save the results in new colums with the same name follow by **_STAND**. Plot a histogram for each new variable.\n",
    "\n",
    "Tip: use [StandardScaler()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to standarize a variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing a dataset refers to rescaling each observation (row) to have a length of 1 (called a unit norm or a vector with the length of 1 in linear algebra).\n",
    "\n",
    "[**CODE**] Normalize the **DATA_TRAFFIC_MONTH_1**, **VOICE_TRAFFIC_MONTH_1**, **BILLING_MONTH_1** and **DEVICE_COST_MONTH_1** columns. Save the results in new colums with the same name follow by **_NORM**. Plot a histogram for each new variable.\n",
    "\n",
    "\n",
    "Tip: use [Normalizer()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html) to normalize a variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When your data is comprised of attributes with varying scales, many machine learning algorithms can benefit from rescaling the attributes to all have the same scale. Often this is referred to as normalization and attributes are often rescaled into the range between 0 and 1.\n",
    "\n",
    "[**CODE**] Rescale the **DATA_TRAFFIC_MONTH_1**, **VOICE_TRAFFIC_MONTH_1**, **BILLING_MONTH_1** and **DEVICE_COST_MONTH_1** columns. Save the results in new colums with the same name follow by **_RESC**. Plot a histogram for each new variable.\n",
    "\n",
    "Tip: use [MinMaxScaler()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) to rescale a variable between max and min values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Convert categorical columns to dummy binary variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical variables transformation is a key stage before any machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**CODE**] Convert **PREVIOUS_DEVICE_BRAND** to a new integer variable with name **PREVIOUS_DEVICE_BRAND_INTEGER_ENCOD** and dummy binary variables. Convert **PREVIOUS_DEVICE_MODEL** to a new integer variable with name **PREVIOUS_DEVICE_MODEL_INTEGER_ENCOD** and dummy binary variables.\n",
    "\n",
    "Tip: use [LabelEncoder()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) to transform a categorical variable to integer values. Use [OneHotEncoder()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) after integer encoding (i.e. [LabelEncoder()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)) to transform a categorical variable into integer values and finally into a one hot value. You can also use [get_dummies()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) instead of OneHotEncoder()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Feature generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the current dataset we have a historic of 6 months for data traffic, voice traffic, billing and device cost. Feature engineering consists of creating new attributes from the current dataset that can help us to improve the performance of a ulterior model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**REPORT**] For the 6 months of **DATA_TRAFFIC**, **VOICE_TRAFFIC**, **BILLING** and **DEVICE_COST** calculate new columns or attributes with the mean, maximum, minimum, range (i.e. difference between maximum and minimum) for each purchase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**CODE**] Create an additional column **DEVICE_COST_TO_BILLING_RATIO** containing the ratio between **DEVICE_COST_MEAN** and **BILLING_MEAN** and plot its distribution.\n",
    "\n",
    "[**REPORT**] Indicate what is the distribution of the variable **DEVICE_COST_TO_BILLING_RATIO**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Text parsing/processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, text processing is a very useful tool that can be used to improve datasets. In some use cases, for instance customer care applications using digital channels as Whatsapp, Facebook, etc..., data scientist teams mainly works with text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the text processing technique is to extract concrete words or tokens from a sentence or documents. Regex is an open source tool with plenty of utilities to extract data from patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**CODE**] The **PURCHASED_DEVICE** is a variable that is formed by a \"**device_code**\"+\"**_**\"+\"**manufacture name**\"+\"**  **\"+\"**device model**\". Applying text processing techniques to **PURCHASED_DEVICE** variable, create 3 new columns with the following variables names: **PURCHASED_DEVICE_CODE**, **PURCHASED_DEVICE_MANUFACTURE** and **PURCHASED_DEVICE_MODEL**.\n",
    "\n",
    "Tip: use [str.split](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.split.html) to separate a string into several parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**REPORT**] Create a table with the number of devices per manufacturer in **PURCHASED_DEVICE_MANUFACTURE** vs **PREVIOUS_DEVICE_MANUF**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Splitting and sampling a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting and sampling dataset are techniques that distribute the original dataset in n-parts. One of the most interesting application of these tools is to separate the dataset to train and test a machine learning model. Meanwhile sampling guarantees same type of data (i.e. distributions), splitting will separate the dataset with the ratio we need. Usually, 80%-20% or 70%-30% splitting ratios are the most common used.\n",
    "\n",
    "Once again, Sklearn library helps to us to cover this necessity: Sklearn provides the function **train_test_split** to  split and sample a dataset in two parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**CODE**] Split the dataset in two separate datasets: one with 80% of the samples and the other with 20% of registers\n",
    "\n",
    "Tip: you can use the [sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function to split into training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**REPORT**] Create a table and compare the main statistics (mean, standard deviation, min, max, 25%, 50%, 75%) of **DATA_TRAFFIC_MONTH_1**, **VOICE_TRAFFIC_MONTH_1** and **BILLING_MONTH_1** in both datasets. Are they similar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Deliver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deliver:\n",
    "\n",
    "* A zip file containing your notebook (.ipynb file) with all the [**CODE**] parts implemented.\n",
    "* A 4-pages PDF report including all parts of this notebook marked with \"[**REPORT**]\". If some table is too large to fit the report, summarize it by, for instance, including in the report only the first few rows.\n",
    "\n",
    "The report should end with the following statement: **I hereby declare that, except for the code provided by the course instructors, all of my code, report, and figures were produced by myself.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
