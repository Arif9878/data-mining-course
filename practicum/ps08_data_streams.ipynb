{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Session 08: Data streams\n",
    "\n",
    "In this session we will take a large corpus of queries and compute statistics on them using methods for data stream sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Preliminaries\n",
    "\n",
    "The dataset we will use contains the most 1,000 prolific users from the AOL Query Log (2006), a dataset released for research, and later retracted [[download link](https://github.com/wasiahmad/aol_query_log_analysis)]. The idea of this practice is to obtain some statistics on this file **without** storing parts of the file in main memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1. Required imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import csv\n",
    "import math\n",
    "import random\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2. How to iterate through this file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On 2006-03-04 13:03:17 user u1142189 issued query yesterdays tractor\n",
      "On 2006-03-05 16:31:08 user u7011826 issued query lumina therapy clinic\n",
      "On 2006-03-16 17:09:07 user u2544636 issued query bud and alley's seaside fl\n",
      "On 2006-03-24 08:12:59 user u4633322 issued query haaretz\n",
      "On 2006-03-26 21:38:42 user u8555813 issued query google\n",
      "On 2006-03-28 15:27:39 user u4555258 issued query gold country casino\n",
      "On 2006-03-28 23:46:55 user u351790 issued query red bull health drink\n",
      "On 2006-03-29 22:51:54 user u8408524 issued query silverthorn resort lake shasta\n",
      "On 2006-03-30 05:12:53 user u6754960 issued query so yesterday lyrics\n",
      "On 2006-03-30 08:13:34 user u4205230 issued query yahoo\n",
      "On 2006-04-02 12:33:58 user u8439139 issued query clive owen\n",
      "On 2006-04-08 15:34:12 user u8221292 issued query history of plessey v. ferguson\n",
      "On 2006-04-08 16:09:15 user u3273964 issued query mens warehouse\n",
      "On 2006-04-08 19:21:37 user u6100295 issued query suffolk horse\n",
      "On 2006-04-08 20:39:48 user u825602 issued query fhm\n",
      "On 2006-04-17 07:43:48 user u6419082 issued query mother's day tea invitations to print\n",
      "On 2006-04-18 22:12:19 user u6284941 issued query mime dance ministry\n",
      "On 2006-04-22 00:19:52 user u665643 issued query mathis brothers\n",
      "On 2006-04-24 04:10:20 user u20667 issued query patek phillipe\n",
      "On 2006-04-24 13:34:12 user u3332758 issued query city of kalamazoo\n",
      "On 2006-04-26 20:45:20 user u7545476 issued query cheap indian boyfriend\n",
      "On 2006-04-28 09:05:05 user u5254221 issued query scooters for handicaps\n",
      "On 2006-04-29 14:51:28 user u3972320 issued query verizon wireless\n",
      "On 2006-05-02 19:03:06 user u2357546 issued query pete wentz myspace icons\n",
      "On 2006-05-09 04:28:53 user u680325 issued query w. 58th st new york\n",
      "On 2006-05-09 18:50:29 user u5235706 issued query century 21\n",
      "On 2006-05-11 12:58:31 user u8337516 issued query adam4adam\n",
      "On 2006-05-15 09:49:45 user u9405785 issued query medical sales representative\n",
      "On 2006-05-18 21:16:29 user u975359 issued query exterior house stain colors\n",
      "On 2006-05-20 20:26:34 user u4831762 issued query planetary reduction gears\n",
      "On 2006-05-21 22:42:04 user u2508913 issued query nautical coloring pages\n",
      "On 2006-05-22 08:56:38 user u4774745 issued query home depot corporation\n",
      "On 2006-05-22 22:22:34 user u2128897 issued query pictures of crossdresser all dress up in women clothes\n",
      "On 2006-05-23 16:35:04 user u3537232 issued query vh1.com\n",
      "On 2006-05-25 22:51:19 user u7716753 issued query eteamz\n",
      "On 2006-05-28 19:48:07 user u7943335 issued query picture or clip art of jesus blessing children\n",
      "On 2006-05-29 14:52:52 user u5653981 issued query german military ranks\n",
      "On 2006-05-30 08:49:11 user u4959954 issued query sahibinden satilik\n"
     ]
    }
   ],
   "source": [
    "INPUT_FILE = \"user_queries.csv\"\n",
    "with io.open(INPUT_FILE) as file:\n",
    "    reader = csv.reader(file, delimiter=\"\\t\")\n",
    "    for timestamp, userid, query in reader:\n",
    "        # Prints 0.01% of lines\n",
    "        if random.random() < 0.0001:\n",
    "            print(\"On %s user %s issued query %s\" % (timestamp, userid, query)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Determine approximately the top-5 queries\n",
    "\n",
    "In this query log the most frequent queries are:\n",
    "\n",
    "* \"google\" (1.6% of the queries)\n",
    "* \"ebay\" (1.5%)\n",
    "* \"yahoo\" (1.3%)\n",
    "* \"myspace\" (1.0%)\n",
    "* \"craigslist\" (0.5%)\n",
    "\n",
    "Instead of loading the entire query log in main memory, we will use reservoir sampling to determine approximately the top-5 queries.\n",
    "\n",
    "**Reservoir sampling**: In reservoir sampling, if we have a reservoir of size S:\n",
    "\n",
    "* We store the first S elements of the stream\n",
    "* When the n<sup>th</sup> element arrives (let's call it X<sub>n</sub>):\n",
    "   * With probability 1 - s/n, we ignore this element.\n",
    "   * With probability s/n, we:\n",
    "      * Discard a random element from the reservoir\n",
    "      * Add element X<sub>n</sub> to the reservoir (calling *add_to_reservoir*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**CODE**] Implement a function `add_reservoir(reservoir, item, max_size)` that adds an item to the reservoir, maintaining its size. If the reservoir is already of size *max_size*, a random item is selected and evicted *before* adding the item. It is important to evict an old item *before* adding the new item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_reservoir(reservoir, item, max_reservoir_size):\n",
    "    # YOUR CODE HERE\n",
    "    assert(len(reservoir) <= max_reservoir_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**CODE**] Iterate through the file using the reservoir sampling method seen in class. In this function you will decide, for every item, whether to call *add_to_reservoir* or to ignore the item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of queries seen    : 318023\n",
      "Number of queries sampled : 500\n"
     ]
    }
   ],
   "source": [
    "def reservoir_sampling(filename, reservoir_size)\n",
    "    reservoir = []\n",
    "\n",
    "    with io.open(filename) as file:\n",
    "        reader = csv.reader(file, delimiter=\"\\t\")\n",
    "        i = 0\n",
    "        for timestamp, userid, query in reader:\n",
    "            i += 1\n",
    "            # YOUR CODE HERE: decide whether to call add_to_reservoir or not\n",
    "            \n",
    "    return i, reservoir\n",
    "\n",
    "num_lines, reservoir = reservoir_sampling(INPUT_FILE, 500)\n",
    "\n",
    "print(\"Number of queries seen    : %d\" % num_lines)\n",
    "print(\"Number of queries sampled : %d\" % len(reservoir) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**CODE**] Write code to list the top-5 queries found by looking at frequencies in the reservoir. If you see a query C times in the reservoir, you can estimate the query appears *C x dataset_size / reservoir_size* times in the entire dataset (*dataset_size* is the size of the entire dataset). Dividing this quantity by *dataset_size* and multiplying by 100 will give you the percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ebay 2.0%\n",
      "myspace 1.8%\n",
      "google 1.6%\n",
      "msn 0.8%\n",
      "yahoo mail 0.6%\n"
     ]
    }
   ],
   "source": [
    "freq = {}\n",
    "for item in reservoir:\n",
    "    freq[item] = reservoir.count(item)\n",
    "\n",
    "most_frequent_items = sorted([(frequency, query) for query, frequency in freq.items()], reverse=True)[:5]\n",
    "    \n",
    "# YOUR_CODE_HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**REPORT**] For various sizes of the reservoir: 10, 20, 50, 100, 200, 500, 1000, 2000, 5000, list the top-5 queries and their estimated frequency. \n",
    "\n",
    "[**REPORT**] Find by trial and error, and include in your report, the minimum reservoir size you would have to use to have an overlap of 3/5 between the queries found by the approximate method and the actual top-5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Determine approximately the number of users\n",
    "\n",
    "We will estimate the number of distinct users without creating a dictionary or hash table with users, but instead, we will use the Flajolet-Martin probabilistic counting method.\n",
    "\n",
    "**Flajolet-Martin probabilistic counting**:\n",
    "\n",
    "* For every element *u* in the stream:\n",
    "   * Compute hash value *h(u)*\n",
    "   * Let *r(u)* be the number of trailing zeroes in *h(u)*\n",
    "   * Maintain *R* as the maximum value of *r(u)* seen so far\n",
    "* Output *2<sup>R</sup>* as our estimate for the number of distinct elements *u* seen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this function to count trailing zeroes in the binary representation of a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trailing_zeroes(number):\n",
    "    count = 0\n",
    "    while number & 1 == 0:\n",
    "        count += 1\n",
    "        number = number >> 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this function to generate a random hash function. Note this generates a function, so you can do `hash_function = random_hash_function()` and then call `hash_function(x)` to compute the hash value of `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_hash_function():\n",
    "    salt = random.random()\n",
    "    return lambda string: hash(string + str(salt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**CODE**] Perform *number_of_passes* passes over the file, reading the entire file on each pass (we don't use the reservoir in this part). In each pass, create a new hash function and use it to hash userids. Keep the maximum number of trailing zeroes seen in the hash value of a userid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimate on pass 1: 512\n",
      "Estimate on pass 2: 512\n",
      "Estimate on pass 3: 256\n",
      "Estimate on pass 4: 8192\n",
      "Estimate on pass 5: 8192\n",
      "Estimate on pass 6: 1024\n",
      "Estimate on pass 7: 128\n",
      "Estimate on pass 8: 4096\n",
      "Estimate on pass 9: 1024\n",
      "Estimate on pass 10: 1024\n",
      "* Average of estimates: 2496.0\n",
      "* Median  of estimates: 1024.0\n"
     ]
    }
   ],
   "source": [
    "number_of_passes = 10\n",
    "\n",
    "estimates = []\n",
    "\n",
    "for i in range(number_of_passes):\n",
    "    # YOUR_CODE_HERE: read the file and generate an estimate\n",
    "    \n",
    "    estimates.append(estimate)\n",
    "    print(\"Estimate on pass %d: %d\" % (i+1, estimate))\n",
    "    \n",
    "print(\"* Average of estimates: %.1f\" % statistics.mean(estimates))\n",
    "print(\"* Median  of estimates: %.1f\" % statistics.median(estimates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**REPORT**] Include in your report the median of estimates obtained in 3 separate runs of your algorithm; each run should do 10 passes over the file. Indicate why the median of estimates is preferable to the average of estimates.\n",
    "\n",
    "*Note: in this dataset, the actual number of users is 1000.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Deliver\n",
    "\n",
    "Deliver:\n",
    "\n",
    "* A zip file containing your notebook (.ipynb file) with all the [**CODE**] parts implemented.\n",
    "* A 2-pages PDF report including all parts of this notebook marked with \"[**REPORT**]\"\n",
    "The report should end with the following statement: **I hereby declare that, except for the code provided by the course instructors, all of our code, report, and figures were produced by myself.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
