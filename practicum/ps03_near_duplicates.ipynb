{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Session 03: Find near-duplicates using shingling\n",
    "\n",
    "In this session we will take a large corpus of tweets and detect near-duplicates on this corpus using a technique known as *shingling*.\n",
    "\n",
    "Two documents are considered near-duplicates if they share a large amount of ngrams. The *ngrams* of a phrase are overlapping sequences of words of length *n*. For instance, the phrase '*My name is Inigo Montoya. You killed my father. Prepare to die.*' has the following 3-grams:\n",
    "\n",
    "* 'my name is'\n",
    "* 'name is inigo'\n",
    "* 'is inigo montoya'\n",
    "* ...\n",
    "* 'my father prepare'\n",
    "* 'father prepare to'\n",
    "* 'prepare to die'\n",
    "\n",
    "To measure the similarity between two sets, we will use the [Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index), which is the size of the intersection of the two sets divided by their union. This values goes between 0.0 (meaning the documents have no ngrams in common) to 1.0 (meaning the documents have the same ngrams).\n",
    "\n",
    "To speed up things, instead of comparing the set of shingles of two documents which can be large, we will derive a fixed-length *signature* or *sketch* for each document. This will be obtained by (1) applying a random permutation to the list of possible ngrams, and (2) pick the ngram that appears first in the permuted list. The Jaccard index between these signatures will be a good approximation of the Jaccard index between the original sets of ngrams. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Preliminaries\n",
    "\n",
    "The corpus you will use is a file named `EstamosPorTi.json.gz`. The file contain about 16,800 Twitter messages (\"tweets\") posted between October 1st, 2017, and October 24th, 2017, and using the hashtag `#EstamosPorTi`. Background information on this collection is available on [the Internet Archive](https://archive.org/details/EstamosporTIOohmm2018032618831Ids). In Twitter there are many near-duplicates because of re-tweets and because people quote each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Required imports\n",
    "\n",
    "We will use the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import gzip\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Read the tweet content into an array of messages\n",
    "\n",
    "The tweets are in a format known as [JSON](https://en.wikipedia.org/wiki/JSON#Example). Python's JSON library takes care of translating it into a dictionary. Finally, this is a gzip-compressed file, which you can de-compress using the `gunzip` command, or read compressed using the `gzip` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 16853 documents\n"
     ]
    }
   ],
   "source": [
    "INPUT_FILENAME = \"EstamosPorTi.json.gz\"\n",
    "\n",
    "messages = []\n",
    "\n",
    "with gzip.open(INPUT_FILENAME, \"rt\", encoding=\"utf-8\") as input_file:\n",
    "    for line in input_file:\n",
    "        tweet = json.loads(line)\n",
    "        author = tweet[\"user\"][\"screen_name\"]\n",
    "        message = tweet[\"full_text\"]\n",
    "        messages.append(message)\n",
    "        \n",
    "print(\"Read %d documents\" % len(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the next line if you want to work with a smaller set temporarily, this might help you during development\n",
    "\n",
    "#messages = [\"hello I am your neighbor\", \"hello I am john\", \"I am john the carpenter\", \"john the carpenter is your neighbor\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Implement auxiliary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**CODE**] Implement the Jaccard similarity between two lists: the size of the intersection of two sets, divided by the size of their union. Tip: use `set(l)` to convert list `l` to a set. Then you can use `s1.union(s2)` and `s1.intersection(s2)` if `s1` and `s2` are sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(list1, list2):\n",
    "    # REPLACE THIS COMMENT WITH YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(jaccard_similarity(['a', 'b', 'c'], ['a', 'c', 'd']))\n",
    "print(jaccard_similarity(['a', 'b', 'c'], ['d', 'e', 'f']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**CODE**] Implement a simple function to clean-up text by converting to lowercase and removing anything that is not a letter or digit. Tip: use `text.lower()` to convert to lowercase, and `re.sub(...)` to replace parts of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    # REPLACE THIS COMMENT WITH YOUR CODE\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am iñigo montoya\n"
     ]
    }
   ],
   "source": [
    "print(clean(\"I am Iñigo Montoya!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implement an n-gram extractor\n",
    "\n",
    "[**CODE**] Implement the function `ngrams(text,size)`, which should produce all sub-sequences of `size` words present in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(text, size):\n",
    "    tokens = text.split()\n",
    "    ngrams = []\n",
    "    # REPLACE THIS COMMENT WITH YOUR CODE\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Docid   : 0\n",
      "Message : rt lavanguardia último tuit del ministerio de interior catalanreferendum httpstcowk33loawtu\n",
      "Ngrams  : [rt lavanguardia último, lavanguardia último tuit, último tuit del, tuit del ministerio, del ministerio de, ministerio de interior, de interior catalanreferendum, interior catalanreferendum httpstcowk33loawtu]\n",
      "\n",
      "Docid   : 1\n",
      "Message : rt julianassange heres spains interior ministry bragging that its military police have have sabotaged access to the catalan government\n",
      "Ngrams  : [rt julianassange heres, julianassange heres spains, heres spains interior, spains interior ministry, interior ministry bragging, ministry bragging that, bragging that its, that its military, its military police, military police have, police have have, have have sabotaged, have sabotaged access, sabotaged access to, access to the, to the catalan, the catalan government]\n",
      "\n",
      "Docid   : 2\n",
      "Message : rt julianassange heres spains interior ministry bragging that its military police have have sabotaged access to the catalan government\n",
      "Ngrams  : [rt julianassange heres, julianassange heres spains, heres spains interior, spains interior ministry, interior ministry bragging, ministry bragging that, bragging that its, that its military, its military police, military police have, police have have, have have sabotaged, have sabotaged access, sabotaged access to, access to the, to the catalan, the catalan government]\n"
     ]
    }
   ],
   "source": [
    "for docid in range(3):\n",
    "    print()\n",
    "    print(\"Docid   : %d\" % docid)\n",
    "    print(\"Message : %s\" % clean(messages[docid]))\n",
    "    print(\"Ngrams  : [%s]\" % \", \".join(ngrams(clean(messages[docid]), 3)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Estimate how long it would take to compare all messages\n",
    "\n",
    "Use the following code to estimate how long would it take to perform pair-wise comparisons between the sets of ngrams of all the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 100\n",
    "\n",
    "start = timer()\n",
    "\n",
    "for docid1 in range(len(messages)):\n",
    "    if docid1 > limit:\n",
    "        break\n",
    "        \n",
    "    doc1 = clean(messages[docid1])\n",
    "    ngrams1 = ngrams(doc1, 3)\n",
    "    \n",
    "    for docid2 in range(docid1+1, len(messages)):\n",
    "        doc2 = clean(messages[docid2])\n",
    "        ngrams2 = ngrams(doc2, 3)\n",
    "        \n",
    "        similarity = jaccard_similarity(ngrams1, ngrams2)\n",
    "            \n",
    "end = timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**REPORT**] Include the following time estimations in your report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning 100 messages took 0:00:23.308696\n",
      "Scanning 16853 messages would take 1:05:28.214606\n"
     ]
    }
   ],
   "source": [
    "print(\"Scanning %d messages took %s\" % (limit, datetime.timedelta(seconds=end-start)))\n",
    "print(\"Scanning %d messages would take %s\" % (len(messages), datetime.timedelta(seconds=(end-start)*len(messages)/limit)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Compute doc-ngram matrix\n",
    "\n",
    "Now we will compute a matrix in which every row is an ngram, and every column is a document.\n",
    "\n",
    "This normally done by hashing the ngrams and then every row is an ngram *hash*; in this practice we will skip that step and work directly with one ngram per row, which is conceptually the same and easier to code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Create list of all ngrams\n",
    "\n",
    "[**CODE**] Implement code to create the dictionary `ngram_to_index`, which should convert an ngram to an index (a row number) and to create the variable `num_distinct_ngrams` which should contain the number of distinct ngrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_size = 3\n",
    "\n",
    "ngram_to_index = {}\n",
    "next_index = 0\n",
    "\n",
    "for message in messages:\n",
    "    all_ngrams = ngrams(clean(message), ngram_size)\n",
    "    for ngram_str in all_ngrams:\n",
    "        if ngram_str not in ngram_to_index:\n",
    "            # REPLACE THIS COMMENT WITH YOUR CODE\n",
    "            \n",
    "num_distinct_ngrams = next_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function. The exact number of total n-grams may vary depending on how you `clean()` text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of 3-grams: 19720\n",
      "15230\n",
      "14689\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of %d-grams: %d\" % (ngram_size, num_distinct_ngrams))\n",
    "print(ngram_to_index[\"hoy a dormir\"])\n",
    "print(ngram_to_index[\"universal census system\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Create table ngrams x documents\n",
    "\n",
    "In the matrix `M_ngram_doc`, each row should be an n-gram, and each column should be a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 19720 (distinct shingles)\n",
      "Columns: 16853 (distinct documents)\n"
     ]
    }
   ],
   "source": [
    "M_ngram_doc = np.zeros((num_distinct_ngrams, len(messages)))\n",
    "print(\"Rows: %d (distinct shingles)\" % len(M_ngram_doc))\n",
    "print(\"Columns: %d (distinct documents)\" % len(M_ngram_doc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**CODE**] Complete the matrix `M_ngram_doc` so that position i, j (row, column) holds a 1 if document j contains ngram i, otherwise holds a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for docid in range(len(messages)):\n",
    "    message = messages[docid]\n",
    "    all_ngrams = ngrams(clean(message), ngram_size)\n",
    "    for ngram_str in all_ngrams:\n",
    "        # REPLACE THIS COMMENT WITH YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check a couple of documents (columns). All columns should be very sparse, i.e., mostly zeroes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positions of non-zeros in column of docid 28 of M_ngram_doc\n",
      "Message   : rt policia agentes intervienen urnas y papeletas en distintos colegios de barcelona cumplimos la legalidad estamosporti httpstco\n",
      "Non-zeros : [63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77]\n",
      "\n",
      "Positions of non-zeros in column of docid 3689 of M_ngram_doc\n",
      "Message   : rt noticialibre agentes policia  intervienen urnas y papeletas en distintos colegios de barcelona cumplimos la legalidad estamospor\n",
      "Non-zeros : [66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 2759, 2760, 2761, 2762, 2763]\n"
     ]
    }
   ],
   "source": [
    "print(\"Positions of non-zeros in column of docid 28 of M_ngram_doc\")\n",
    "print(\"Message   : %s\" % clean(messages[28]))\n",
    "ngrams_doc_28 = [i for i in range(num_distinct_ngrams) if M_ngram_doc[i, 28] == 1.0]\n",
    "print(\"Non-zeros : %s\" % ngrams_doc_28)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Positions of non-zeros in column of docid 3689 of M_ngram_doc\")\n",
    "print(\"Message   : %s\" % clean(messages[3689]))\n",
    "ngrams_doc_3689 = [i for i in range(num_distinct_ngrams) if M_ngram_doc[i, 3689] == 1.0]\n",
    "print(\"Non-zeros : %s\" % ngrams_doc_3689)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Implement a permutation generator\n",
    "\n",
    "[**CODE**] Implement the function `random_permutation(k)`, which should generate a random permutation of the array `[0, 2, 3, ..., k-1]`. Tip: the function `random.shuffle(...)` might be useful, and if you want to use `range(...)`, which returns an iterator, you will need to convert it to a list using `list(range(...))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_permutation(k):\n",
    "    # REPLACE THIS COMMENT WITH YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function. Every number between *0* and *k-1* should be in the permutation once, and every call to the function should generate a new permutation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 4, 13, 8, 18, 14, 3, 7, 2, 12, 10, 17, 1, 16, 9, 11, 19, 0, 6, 5]\n",
      "[0, 5, 14, 9, 10, 7, 17, 15, 4, 6, 12, 1, 19, 13, 11, 3, 16, 2, 18, 8]\n"
     ]
    }
   ],
   "source": [
    "print(random_permutation(20))\n",
    "print(random_permutation(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further test this by applying the same permutation on two lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c', 'd', 'b', 'a', 'e']\n",
      "['gamma', 'delta', 'beta', 'alpha', 'epsilon']\n",
      "['c', 'a', 'd', 'b', 'e']\n",
      "['gamma', 'alpha', 'delta', 'beta', 'epsilon']\n"
     ]
    }
   ],
   "source": [
    "def permuter(original_list, permutation):\n",
    "    permuted_list = []\n",
    "    for index in permutation:\n",
    "        permuted_list.append(original_list[index])\n",
    "    return permuted_list\n",
    "\n",
    "original_list_1 = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "original_list_2 = [\"alpha\", \"beta\", \"gamma\", \"delta\", \"epsilon\"]\n",
    "\n",
    "permutation_1 = random_permutation(5)\n",
    "print(permuter(original_list_1, permutation_1))\n",
    "print(permuter(original_list_2, permutation_1))\n",
    "\n",
    "permutation_2 = random_permutation(5)\n",
    "print(permuter(original_list_1, permutation_2))\n",
    "print(permuter(original_list_2, permutation_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Compute the signature of each document\n",
    "\n",
    "Now comes the core of the algorith. We will create a new matrix `M_signature_doc` having a small number of rows (the *signature size*), which will be equivalent to the number of permutations we use. The number of columns will continue being the number of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_permutations = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**CODE**] Create *num_permutation* permutations and store them in the array of arrays `permutations`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 4 permutations for 19720 distinct ngrams\n"
     ]
    }
   ],
   "source": [
    "permutations = []\n",
    "\n",
    "print(\"Creating %d permutations for %d distinct ngrams\" % (num_permutations, num_distinct_ngrams))\n",
    "for i in range(num_permutations):\n",
    "    # REPLACE THIS COMMENT WITH YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test by printing the permutations you have created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Permutation 0: 209, 5629, 7280, ...\n",
      "Permutation 1: 7678, 5824, 2141, ...\n",
      "Permutation 2: 4741, 16299, 14862, ...\n",
      "Permutation 3: 10075, 10946, 599, ...\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(permutations)):\n",
    "    permutation = permutations[i]\n",
    "    print(\"Permutation %d: %d, %d, %d, ...\" % (i, permutation[0], permutation[1], permutation[2] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**CODE**] Implement the signature construction. The matrix `M_signature_doc` should contain in row i, column j, the first ngram (the \"minimum\" one) that is present in a column (document), according to the order given by a permutation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating signatures for permutation 0/4\n",
      "Creating signatures for permutation 1/4\n",
      "Creating signatures for permutation 2/4\n",
      "Creating signatures for permutation 3/4\n"
     ]
    }
   ],
   "source": [
    "M_signature_doc = np.zeros((num_permutations, len(messages)))\n",
    "\n",
    "def find_smallest_ngram_permutation_order(docid, permutation):\n",
    "    for ngram_id in permutation:\n",
    "        if M_ngram_doc[ngram_id, docid] == 1:\n",
    "            return ngram_id\n",
    "    return -1\n",
    "\n",
    "for permutation_num in range(num_permutations):\n",
    "    print(\"Creating signatures for permutation %d/%d\" % (permutation_num, num_permutations))\n",
    "    permutation = permutations[permutation_num]\n",
    "    for docid in range(len(messages)):\n",
    "        # REPLACE THIS COMMENT WITH YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code by checking the signatures of two documents that are near-duplicates. Being near-duplicates, we expect these should have many ngrams in common, and hence, with high probability they will have many elements in common in their signatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signature of document 28\n",
      "Message   : rt policia agentes intervienen urnas y papeletas en distintos colegios de barcelona cumplimos la legalidad estamosporti httpstco\n",
      "Ngrams    : [63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77]\n",
      "Signature : [63.0, 66.0, 77.0, 66.0]\n",
      "\n",
      "Signature of document 3689\n",
      "Message   : rt noticialibre agentes policia  intervienen urnas y papeletas en distintos colegios de barcelona cumplimos la legalidad estamospor\n",
      "Ngrams    : [66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 2759, 2760, 2761, 2762, 2763]\n",
      "Signature : [2759.0, 66.0, 69.0, 66.0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Signature of document 28\")\n",
    "print(\"Message   : %s\" % clean(messages[28]))\n",
    "print(\"Ngrams    : %s\" % ngrams_doc_28)\n",
    "signature_28 = [M_signature_doc[i, 28] for i in range(num_permutations)]\n",
    "print(\"Signature : %s\" % signature_28)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Signature of document 3689\")\n",
    "print(\"Message   : %s\" % clean(messages[3689]))\n",
    "print(\"Ngrams    : %s\" % ngrams_doc_3689)\n",
    "signature_3689 = [M_signature_doc[i, 3689] for i in range(num_permutations)]\n",
    "print(\"Signature : %s\" % signature_3689)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Compare all pair-wise signatures\n",
    "\n",
    "Now we are ready to compare all documents by their signatures, instead of by their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_signature(docid):\n",
    "    return [M_signature_doc[i, docid] for i in range(num_permutations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 100\n",
    "\n",
    "start = timer()\n",
    "\n",
    "for docid1 in range(len(messages)):\n",
    "    if docid1 > limit:\n",
    "        break\n",
    "    \n",
    "    signature1 = extract_signature(docid1)        \n",
    "    for docid2 in range(docid1+1, len(messages)):\n",
    "        signature2 = extract_signature(docid2)\n",
    "        \n",
    "        similarity = jaccard_similarity(signature1, signature2)\n",
    "        \n",
    "end = timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**REPORT**] Include the following time estimations in your report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning 100 took 0:00:04.246620\n",
      "Scanning 16853 would take 0:11:55.682856\n"
     ]
    }
   ],
   "source": [
    "print(\"Scanning %d took %s\" % (limit, datetime.timedelta(seconds=end-start)))\n",
    "print(\"Scanning %d would take %s\" % (len(messages), datetime.timedelta(seconds=(end-start)*len(messages)/limit)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Compute percentage of pairs having the following similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**CODE**] [**REPORT**] Implement code to compute and report: what percentage of pairs have similarity strictly greater than 0.5 but strictly less than 1.0; give 2-3 examples of them (include docids and texts).\n",
    "\n",
    "[**CODE**] [**REPORT**] Implement code to compute and report: what percentage of pairs of documents have similarity 1.0, give 2-3 examples of them (include docids and texts).\n",
    "\n",
    "[**CODE**] [**REPORT**] Implement code to compute and report: what is the document that has more duplicates (give the docid and its text).\n",
    "\n",
    "Idea: you can save time by implementing these three in a single pass doing two nested loops over the messages (as above), instead of implementing these as three separate passes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Deliver\n",
    "\n",
    "Deliver:\n",
    "\n",
    "* A zip file containing your notebook (.ipynb file) with all the [**CODE**] parts implemented.\n",
    "* A 1-page PDF report including all parts of this notebook marked with \"[**REPORT**]\"\n",
    "\n",
    "The report should end with the following statement: **I hereby declare that, except for the code provided by the course instructors, all of our code, report, and figures were produced by myself.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
